{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLi-REC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMk+ZGzPcDMZjkU2qae3wGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wendy60/Hybrid-recommender-system/blob/second-submit/SLi_REC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use an open source project from github, so I need to declare the copyright for each model. I use the MIND public dataset and the python package -- recommenders from microsoft.\n",
        "\n",
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ],
      "metadata": {
        "id": "q8hyhm3Ws8rC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Global Settings and Imports**"
      ],
      "metadata": {
        "id": "NqImzbDJtBLA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlL7mNWmc5Ox",
        "outputId": "3cc5ea66-635d-4aa6-b719-d2978d81d152"
      },
      "source": [
        "pip install recommenders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting recommenders\n",
            "  Downloading recommenders-0.7.0-py3-none-manylinux1_x86_64.whl (314 kB)\n",
            "\u001b[K     |████████████████████████████████| 314 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba<1,>=0.38.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.51.2)\n",
            "Collecting transformers<5,>=2.5.0\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.19.5)\n",
            "Collecting lightfm<2,>=1.15\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[K     |████████████████████████████████| 310 kB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2<3,>=2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.11.3)\n",
            "Requirement already satisfied: matplotlib<4,>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.2.2)\n",
            "Collecting nltk<4,>=3.4\n",
            "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 62.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5,>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (4.62.3)\n",
            "Collecting pydocumentdb>=2.3.3<3\n",
            "  Downloading pydocumentdb-2.3.5-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml<6,>=5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 47.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: bottleneck<2,>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.2)\n",
            "Collecting scikit-surprise<=1.1.1,>=0.19.1\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 13.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.23.0)\n",
            "Requirement already satisfied: lightgbm>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.2.3)\n",
            "Requirement already satisfied: seaborn<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.11.2)\n",
            "Collecting cornac<2,>=1.1.2\n",
            "  Downloading cornac-1.14.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 42.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.3)\n",
            "Collecting category-encoders<2,>=1.3.0\n",
            "  Downloading category_encoders-1.3.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<2,>1.0.3 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.1.5)\n",
            "Collecting memory-profiler<1,>=0.54.0\n",
            "  Downloading memory_profiler-0.58.0.tar.gz (36 kB)\n",
            "Collecting pymanopt<1,>=0.2.5\n",
            "  Downloading pymanopt-0.2.5-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting scikit-learn<1,>=0.22.1\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.5.2)\n",
            "Collecting powerlaw\n",
            "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3,>=2->recommenders) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (3.0.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler<1,>=0.54.0->recommenders) (5.4.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (7.1.2)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>1.0.3->recommenders) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->category-encoders<2,>=1.3.0->recommenders) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1,>=0.22.1->recommenders) (3.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (4.8.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers<5,>=2.5.0->recommenders) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=2.5.0->recommenders) (3.6.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac<2,>=1.1.2->recommenders) (1.2.1)\n",
            "Building wheels for collected packages: lightfm, memory-profiler, scikit-surprise\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705355 sha256=093e7908b420597961dd630f4bcdaf915b7cd8efeb634770f351d776ed754e28\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-py3-none-any.whl size=30190 sha256=4c30cd526b06da8f71c21b454a0bb5f234c2ff4bef9e9ac80a565fee36cfa2df\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/19/d5/8cad06661aec65a04a0d6785b1a5ad035cb645b1772a4a0882\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1619408 sha256=757033b39723f00883261a9c233c1aa1fb78177a2a011e999657b8bbf7535b8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built lightfm memory-profiler scikit-surprise\n",
            "Installing collected packages: regex, pyyaml, tokenizers, scikit-learn, sacremoses, powerlaw, huggingface-hub, transformers, scikit-surprise, pymanopt, pydocumentdb, nltk, memory-profiler, lightfm, cornac, category-encoders, recommenders\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed category-encoders-1.3.0 cornac-1.14.1 huggingface-hub-0.1.2 lightfm-1.16 memory-profiler-0.58.0 nltk-3.6.5 powerlaw-1.5 pydocumentdb-2.3.5 pymanopt-0.2.5 pyyaml-5.4.1 recommenders-0.7.0 regex-2021.11.10 sacremoses-0.0.46 scikit-learn-0.24.2 scikit-surprise-1.1.1 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MXDlsOlgnoE",
        "outputId": "953357f9-af89-4fc6-8c8e-5dfee3fb832a"
      },
      "source": [
        "pip install tensorflow-gpu==1.15.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu==1.15.2\n",
            "  Downloading tensorflow_gpu-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (410.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 410.9 MB 29 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.8.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 65.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.13.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.37.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.42.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 38.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=9c27cd35ecbc708c633bde795e0c6f4637ea4d1bc3b985f810ca6277bc3ca839\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ5pGpptcTZW",
        "outputId": "d8dd2e97-f7b7-42e5-d2ed-f24c988df16f"
      },
      "source": [
        "import re\n",
        "import sys\n",
        "import os\n",
        "import logging\n",
        "#import papermill as pm\n",
        "#import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.utils.constants import SEED\n",
        "from recommenders.models.deeprec.deeprec_utils import (\n",
        "    prepare_hparams\n",
        ")\n",
        "#from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
        "from recommenders.datasets.download_utils import maybe_download\n",
        "\n",
        "from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n",
        "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System version: 3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n",
            "Tensorflow version: 1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Input dataset**"
      ],
      "metadata": {
        "id": "vWz_ND8ZtMsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly，I need to jump into the correct file path to access the data file in my google drive"
      ],
      "metadata": {
        "id": "MvBckvTZtt3F"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWti56snNWYK",
        "outputId": "4e21688b-827d-42fc-b040-932a59f8c534"
      },
      "source": [
        "#link google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX86GcgHNfpt",
        "outputId": "720c62e2-39d4-4753-822e-86c94d04fa2a"
      },
      "source": [
        "#enter the folder\n",
        "!ls\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/graduation- project/recommenders-main')\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n",
            "AUTHORS.md\t    data\t LICENSE\t recommenders  tests\n",
            "CODE_OF_CONDUCT.md  data1\t MANIFEST.in\t scenarios     tools\n",
            "conda.md\t    docs\t NEWS.md\t SECURITY.md   tox.ini\n",
            "contrib\t\t    examples\t pyproject.toml  SETUP.md      Untitled0.ipynb\n",
            "CONTRIBUTING.md     GLOSSARY.md  README.md\t setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPRlN6zccnTJ"
      },
      "source": [
        "##  config file\n",
        "yaml_file = './recommenders/models/deeprec/config/sli_rec.yaml'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prepare parameters**"
      ],
      "metadata": {
        "id": "ax_NV4O4uqBk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q2Yaru8hALr"
      },
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 400\n",
        "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
        "\n",
        "data_path = os.path.join(\".\", \"data1\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xzmAwdobTD1"
      },
      "source": [
        "train_num_ngs = 4 # number of negative instances with a positive instance for training\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1igKFcPm1cYs"
      },
      "source": [
        "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
        "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
        "cate_vocab = os.path.join(data_path, r'category_vocab.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prepare hyper-parameters**"
      ],
      "metadata": {
        "id": "5Qindw9Lu-o2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD55VaqXhHWW",
        "outputId": "a61bb76e-0ddd-41fc-a510-8b7436f24e43"
      },
      "source": [
        "hparams = prepare_hparams(yaml_file, \n",
        "                          embed_l2=0., \n",
        "                          layer_l2=0., \n",
        "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
        "                          epochs=EPOCHS,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          show_step=20,\n",
        "                          MODEL_DIR=os.path.join(data_path, \"model/\"),\n",
        "                          SUMMARIES_DIR=os.path.join(data_path, \"summary/\"),\n",
        "                          user_vocab=user_vocab,\n",
        "                          item_vocab=item_vocab,\n",
        "                          cate_vocab=cate_vocab,\n",
        "                          need_sample=True,\n",
        "                          train_num_ngs=train_num_ngs, \n",
        "                          # provides the number of negative instances for each positive instance for loss computation.\n",
        "           \n",
        "            )\n",
        "\n",
        "print(hparams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kg_file=None,user_clicks=None,FEATURE_COUNT=None,FIELD_COUNT=None,data_format=None,PAIR_NUM=None,DNN_FIELD_NUM=None,n_user=None,n_item=None,n_user_attr=None,n_item_attr=None,iterator_type=None,SUMMARIES_DIR=./data1/summary/,MODEL_DIR=./data1/model/,wordEmb_file=None,entityEmb_file=None,contextEmb_file=None,news_feature_file=None,user_history_file=None,use_entity=True,use_context=True,doc_size=None,history_size=None,word_size=None,entity_size=None,entity_dim=None,entity_embedding_method=None,transform=None,train_ratio=None,dim=None,layer_sizes=[100, 64],cross_layer_sizes=None,cross_layers=None,activation=['relu', 'relu'],cross_activation=identity,user_dropout=True,dropout=[0.3, 0.3],attention_layer_sizes=None,attention_activation=None,attention_dropout=0.0,model_type=sli_rec,method=classification,load_saved_model=False,load_model_name=None,filter_sizes=None,num_filters=None,mu=None,fast_CIN_d=0,use_Linear_part=False,use_FM_part=False,use_CIN_part=False,use_DNN_part=False,init_method=tnormal,init_value=0.01,embed_l2=0.0,embed_l1=0.0,layer_l2=0.0,layer_l1=0.0,cross_l2=0.0,cross_l1=0.0,reg_kg=0.0,learning_rate=0.001,lr_rs=1,lr_kg=0.5,kg_training_interval=5,max_grad_norm=2,is_clip_norm=0,dtype=32,loss=softmax,optimizer=lazyadam,epochs=10,batch_size=400,enable_BN=True,show_step=20,save_model=True,save_epoch=1,metrics=['auc', 'logloss'],write_tfevents=True,item_embedding_dim=32,cate_embedding_dim=8,user_embedding_dim=16,train_num_ngs=4,need_sample=True,embedding_dropout=0.0,user_vocab=./data1/user_vocab.pkl,item_vocab=./data1/item_vocab.pkl,cate_vocab=./data1/category_vocab.pkl,pairwise_metrics=['mean_mrr', 'ndcg@2;4;6', 'group_auc'],EARLY_STOP=10,max_seq_length=50,hidden_size=40,L=None,T=None,n_v=None,n_h=None,min_seq_length=1,attention_size=40,att_fcn_layer_sizes=[80, 40],dilations=None,kernel_size=None,embed_size=None,n_layers=None,decay=None,eval_epoch=None,top_k=None,slots=5,cell=SUM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create data loader**"
      ],
      "metadata": {
        "id": "kiINVRL8vTlb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0KXklPHioVX"
      },
      "source": [
        "input_creator = SequentialIterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create model**"
      ],
      "metadata": {
        "id": "GxF165KTvaNi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u22QwmYgir5W"
      },
      "source": [
        "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aHG7bfWe_Ki"
      },
      "source": [
        "test_file = os.path.join(data_path, r'final')\n",
        "test_num_ngs = 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the model**"
      ],
      "metadata": {
        "id": "Rwrk48_Zwff3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTOpQFz4iusF",
        "outputId": "cc3e1b20-2e9a-4214-ba3d-84bab5257e41"
      },
      "source": [
        "# test_num_ngs is the number of negative lines after each possitive line in the test_file\n",
        "result = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'auc': 0.5008, 'logloss': 0.6931, 'mean_mrr': 0.6376, 'ndcg@2': 0.5876, 'ndcg@4': 0.6115, 'ndcg@6': 0.6408, 'group_auc': 0.4984}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g_B75PLvnSd",
        "outputId": "c68475a2-9c76-46bc-8b93-a15e480aed76"
      },
      "source": [
        "## Use regular expressions\n",
        "\n",
        "import re\n",
        "tempo = '1004659200\\x021010361600\\x021010361600\\x021010361600\\x021010361600\\x021010361600'\n",
        "tempo = tempo.encode('unicode_escape').decode('utf-8').replace(' ', '')\n",
        "# Use regular matching of special characters starting with \\x\n",
        "result = re.findall(r'\\\\x[a-f0-9]{2}', tempo)\n",
        "for x in result:\n",
        "\t#  Replace special characters found\n",
        "    tempo = tempo.replace(x, '')\n",
        "#  Finally decode\n",
        "tempo = tempo.encode('utf-8').decode('unicode_escape')\n",
        "print(tempo)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100465920010103616001010361600101036160010103616001010361600\n"
          ]
        }
      ]
    }
  ]
}